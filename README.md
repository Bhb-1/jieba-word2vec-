# jieba-word2vec-
新手nlp上路，将中文文本进行分词，并向量化，为深度学习训练数据做准备。

python语言，有python基础即可看明白。

文本数据不大 仅仅是做测试使用，以后有时间会更新完整版

test.py可以忽略，只需要运行word2vec.py就可以 实现将txt文件里的内容进行分词，并且分词后的也都保存到了其他txt文本里，代码里有冗余的地方（主要出现在去停用词、标点符号、数字、英文字母的地方），不过基本功能测试没有问题。

新手nlp上路  欢迎star

代码参考链接：https://blog.csdn.net/qq_28840013/article/details/89681499?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.control

https://blog.csdn.net/sk_berry/article/details/105157066
